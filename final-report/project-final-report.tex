%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title
\title{Project Proposal \\ \\ Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs (CXR-ML-GZSL)}
\author{
    Mario Abdelsayed\\
    Luis Ramos\\
}

\date{\today}

\begin{document}

\maketitle


\section{Abstract}
\subsection{Video Presentation}
TODO: at the end 
\subsection{GitHub Repository}
TODO: at the end
\section{Introduction}

\subsection{Problem Statement}

Deep learning models have been employed in health care in classifying disease and
aiding diagnosis to great effect. One limitation of these models is that they require large
amounts of annotated data that are expensive to acquire. Another limitation the models
suffer from is the inability to assign detect classes absent in the training dataset. It is
difficult to collect data at the volume required to train a model, particularly in the
case of rare and novel diseases.

Zero shot learning (ZSL) has produced some impressive results classifying unseen
classes. Despite that success, the methods proposed only assign one label while in
medical imaging tasks, images often contain more than one disease. This paper proposes a
novel method that combines information from the visual features of images and semantic
embeddings from rich medical text corpus with the goal of assigning multiple labels to natural chest X-Ray images that could be in both seen and unseen classes.



\subsection{Reproducibility}
TODO: At the end

\section{Methodology}

\subsection{Environment}
\begin{itemize}
    \item Python version 3.6.12 was used for this paper
    \item Conda is used to manage the dependencies for this repository. The full list of dependencies is available in the envrionment.yml file. To install them in an environment, simply execute `conda env create -f environment.yml` in the repository directory followed by `conda activate zsl`
    \item One legacy dependency is torvision 0.5.0. You may need to independently install a newer version of the dependency as the version included in the original source code is deprecated and difficult to find
\end{itemize}

\subsection{Data}

\subsubsection{Download}
The data can be downloaded by navigating to the NIH Clinical Center site and clicking on the Download button at the top right of the page (https://nihcc.app.box.com/v/ChestXray-NIHCC)

\subsubsection{Description}
The data is found in the Data\_Entry\_2017\_v2020 csv file. The following table outlines the fields available.


\newblock

\begin{tabular}{|l|p{0.4\linewidth}|}
 \hline
 \textbf{Field} & \textbf{Description} \\
 Image Index & Unique identifier and filename for each image. \\
 Finding Labels & Diagnostic findings present in the image. Multiple labels are separated by `|`. \\
 Follow-up & Indicator for follow-up examinations\\
 Patient ID & Unique identifier assigned to each patient. \\
 Patient Age & Age of the patient at the time of image capture (in years). \\
 Patient Sex & Biological sex of the patient; typically `M` (male) or `F` (female). \\
 View Position & The orientation or view of the X-ray (e.g., PA for posteroanterior). \\
 Original Image Width & The width of the image in pixels. \\
 Original Image Height & The height of the image in pixels. \\
 Original Image Pixel Spacing (x) & The physical spacing (resolution) of a pixel along the x-axis. \\
 Original Image Pixel Spacing (y) & The physical spacing (resolution) of a pixel along the y-axis. \\ 
 \hline
\end{tabular}


\subsection{References and Original Repository}
\begin{itemize}
    \item Hayat, N., Lashen, H., and Shamout, F. E. (2021). Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs. arXiv:2107.06563.
    \item  https://github.com/nyuad-cai/CXR-ML-GZSL?tab=readme-ov-file#Model-training


\section{Model Description}

\subsection{Overview}

The CXR-ML-GZSL model is a multi-label generalized zero-shot learning (ML-GZSL) framework developed to classify both seen and unseen diseases in chest X-ray (CXR) images. Traditional supervised deep learning approaches cannot recognize unseen classes during training, but CXR-ML-GZSL addresses this by mapping both visual and semantic information into a joint latent space, allowing inference on unseen diseases. The model is fully end-to-end trainable and uses semantic features derived from BioBERT and visual features extracted from a DenseNet-121.

\subsection{Architecture Components}

\begin{itemize}
    \item Visual Encoder: A modified DenseNet-121 CNN removes the final classification layer to output a visual feature vector $f_v \in \mathbb{R}^{1024}$ for each image.
    
    \item Visual Projection Module: A 3-layer feedforward neural network projects $f_v$ into a joint latent space:
    \[
    f_l = \text{ReLU}(W_3 (\text{ReLU}(W_2(\text{ReLU}(W_1 f_v + b_1)) + b_2)) + b_3)
    \]
    
    \item Semantic Encoder and Projection: Semantic embeddings $w_i \in \mathbb{R}^d$ are extracted from BioBERT and mapped to the same latent space using a projection network $\psi(w_i)$.
    
    \item  Cosine similarity is used to compute the relevance between the visual and semantic features in the latent space.
\end{itemize}

\subsection{Key Equations and Loss Functions}

\begin{itemize}
    \item Equation 1: The approach used utilizes a Zero-Shot learning method that uses semantic embeddings along with visual features to classify images with unseen classes. Both semantic and visual features go through mapping modules that project the features into the same latent space where a comparison using Cosine similarity can be performed.
    \[P^x_C = (f_x,W)\]
where (,) is the cosine similarity function, \(f_x\) represents the visual features of the input image x, and \(W\) represents the semantic embeddings for all possible classes

\item Equation 2: confidence margin (\( \delta \)) is enforced to mitigate the hub-ness problem. For a label to be assigned to an image, its confidence margin must be larger than delta. 

\[P_(y_p) - P_(y_n) >= \delta\ \]

    \end{itemize}

    \item Equation 3: Latent Space Similarity
    \[
    P_S(x) = \langle \phi(x), \psi(W_S) \rangle
    \]
    where $\phi(x)$ and $\psi(W_S)$ are the projected visual and semantic representations.

    \item Equation 4: Total Loss Function
    \[
    \mathcal{L} = \mathcal{L}_{\text{rank}} + \lambda_1 \mathcal{L}_{\text{align}} + \lambda_2 \mathcal{L}_{\text{con}}
    \]

    \item Equation 5: Ranking Loss
    \[
    \mathcal{L}_{\text{rank}} = \frac{1}{S} \sum_{y_p \in Y^+} \sum_{y_n \in Y^-} \max(0, \delta + p_{y_n} - p_{y_p})
    \]

    \item Equation 6: Alignment Loss
    \[
    \mathcal{L}_{\text{align}} = \frac{1}{N} \sum_{x \in X_s} (1 - \langle \phi(x), \bar{w}_x \rangle)
    \]
    where $\bar{w}_x$ is the averaged semantic embedding of all labels for image $x$.

    \item Equation 7: Semantic Consistency Loss
    \[
    \mathcal{L}_{\text{con}} = \sum_{i \ne j} \left| \langle w_i, w_j \rangle - \langle \psi(w_i), \psi(w_j) \rangle \right|
    \]
\end{itemize}

\subsection{Inputs and Outputs}

Inputs:
\begin{itemize}
    \item $x$: a chest X-ray image.
    \item $y_x \in \{0, 1\}^S$: binary vector of labels.
    \item $W$: semantic embeddings for all classes..
\end{itemize}

Outputs:
\begin{itemize}
    \item $P_c(x) \in \mathbb{R}^C$: similarity scores for seen and unseen disease classes.
    \item Top-k labels: used to compute performance metrics like precision and recall.
\end{itemize}

\subsection{Techniques Used}

\begin{itemize}
    \item Generalized zero-shot learning for prediction of both seen and unseen diseases.
    \item Multi-label classification using cosine similarity scoring.
    \item Domain-specific semantic encoding with BioBERT embeddings.
    \item Joint latent space projection using dedicated projection networks.
    \item Margin-based to prioritize relevant labels over irrelevant ones.
    \item Semantic manifold regularization to preserve inter-class relationships in the semantic space.
\end{itemize}

\subsection{Pre-trained Models}

The model uses two pre-trained models:

\begin{itemize}
    \item DenseNet-121 model was pre-trained on ImageNet and fine-tuned end-to-end during training to serve as the visual encoder.
    \item BioBERT model was trained with biomedical corpora and used to produce semantic mappings.
\end{itemize}

\subsection{References}
\begin{itemize}
    \item Hayat, N., Lashen, H., and Shamout, F. E. (2021). Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs. arXiv:2107.06563.
    \item Lee et al. (2018), Mikolov et al. (2013), Rahman et al. (2020), Rajpurkar et al. (2017)
\end{itemize}

\subsection{Novelty/Relevance/Hypotheses to be Tested}

This paper outlines a novel technique where an end-to-end trainable network jointly
learns the visual and semantic representations of diseases. The technique ensures that
the visual features are well centered around the semantic representations in the
shared latent space. This is the first proposed model that tackles multi-label
generalized zero shot learning on medical imaging.

\subsection{Ablations/Extensions Planned}

A potential extension involves enhancing the semantic representation learning by
incorporating additional medical corpora. Additionally, experimenting with different
loss functions and regularization techniques could improve generalization to
unseen classes. The hypothesis in this paper addresses the real limitation of
traditional models, which are unable to classify diseases not been trained on by
enabling using learned visual and semantic relationships.

\section{Data Access and Implementation Details}

\subsection{Data and Model Access}

The authors have made their original source code publicly available on the GitHub
platform, including scripts to train and evaluate the model. The dataset used in this
project is the NIH Chest X-ray Dataset, which is widely available from different
sources on the web.

\subsection{Feasibility of Computation}

Reproducing the results presented in the paper seems entirely achievable. The authors, for the greater benefit of research, have generously made both the original code and the dataset used to train and evaluate the model publicly accessible. Upon reviewing the code, we determined the authors utilized well-documented libraries and tools, which not only facilitates our understanding of the model but also provides insight into their intentions behind the implementation. Moreover, the computational resources required to replicate the results are readily available, further making the reproduction of the model achievable.

\subsection{Use of Existing Code}

Our primary goal is to leverage the original source code as a foundation for developing a new iteration that can produce the same results. Considering our limited expertise in this particular topic and the constraints of the project's scope, we anticipate that certain elements of the existing code base will be retained in the new iteration. This approach will allow us to build upon the work already done by the original authors while ensuring we achieve the same outcomes.

% \bibliography{aaai25}

\end{document}